{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "QUESTION 6: Basic Gradient Calculations\n",
      "============================================================\n",
      "x.grad = tensor([7.])\n",
      "\n",
      "a) Expected gradient:\n",
      "   y = x² + 3x\n",
      "   dy/dx = 2x + 3\n",
      "   At x=2: dy/dx = 2(2) + 3 = 7\n",
      "   Expected: tensor([7.])\n",
      "\n",
      "b) If requires_grad=False:\n",
      "   x.grad will be None: None\n",
      "   PyTorch doesn't track operations for gradient computation\n",
      "\n",
      "c) Without specifying requires_grad flag:\n",
      "   Default requires_grad = False\n",
      "   Gradients will NOT be tracked (default is False)\n",
      "\n",
      "============================================================\n",
      "QUESTION 7: Introduce Weights\n",
      "============================================================\n",
      "x.grad = tensor([7.])\n",
      "w.grad = tensor([4., 2.])\n",
      "\n",
      "a) Original code (w without requires_grad=True):\n",
      "   w.grad would be None because requires_grad=False by default\n",
      "   The graph wasn't tracking gradients w.r.t. w\n",
      "\n",
      "b) Modified code above shows w.grad computation\n",
      "   w[0].grad = dy/dw[0] = x² = 4\n",
      "   w[1].grad = dy/dw[1] = x = 2\n",
      "   Result: tensor([4., 2.])\n",
      "\n",
      "c) Same as Q6c: Default requires_grad=False, so gradients\n",
      "   are NOT tracked unless explicitly set to True\n",
      "\n",
      "============================================================\n",
      "QUESTION 8: Breaking the Graph\n",
      "============================================================\n",
      "\n",
      "Original code:\n",
      "   x = torch.tensor([1.0], requires_grad=True)\n",
      "   y = x * 3\n",
      "   z = y.detach()\n",
      "   w = z * 2\n",
      "   w.backward()  # ERROR!\n",
      "\n",
      "Why it fails:\n",
      "   - .detach() breaks the computational graph\n",
      "   - z has no gradient information linked to x\n",
      "   - w depends on z, but z doesn't track gradients\n",
      "   - No path exists to backpropagate to x\n",
      "\n",
      "Fix 1: Don't detach if you need gradients\n",
      "   x.grad = tensor([6.])  (should be 6.0)\n",
      "\n",
      "Fix 2: Use detach_() but compute loss without it:\n",
      "\n",
      "============================================================\n",
      "QUESTION 9: Gradient Accumulation\n",
      "============================================================\n",
      "After first backward: x.grad = tensor([2.])\n",
      "After second backward: x.grad = tensor([5.])\n",
      "\n",
      "What's happening:\n",
      "   - First backward: x.grad = 2\n",
      "   - Second backward: x.grad += 3 (accumulated!)\n",
      "   - Result: x.grad = 5 instead of 3\n",
      "\n",
      "Why:\n",
      "   PyTorch ACCUMULATES gradients by default\n",
      "   This is useful for mini-batch training\n",
      "\n",
      "How to avoid:\n",
      "   Option 1: Manually zero gradients\n",
      "   After y1.backward(): x.grad = tensor([2.])\n",
      "   After zero_() and y2.backward(): x.grad = None\n",
      "\n",
      "   Option 2: In training loops, use optimizer.zero_grad()\n",
      "   This is the standard practice in PyTorch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8l/ghc8n7rs36g99m49q1bqfw2h0000gn/T/ipykernel_88735/3159594839.py:123: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/build/aten/src/ATen/core/TensorBody.h:494.)\n",
      "  print(f\"   After zero_() and y2.backward(): x.grad = {y2.grad}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"QUESTION 6: Basic Gradient Calculations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a tensor with requires_grad=True\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "# Define a simple function y = x² + 3x\n",
    "y = x**2 + 3 * x\n",
    "# Backpropagate\n",
    "y.backward()\n",
    "# Print the gradient\n",
    "print(\"x.grad =\", x.grad)\n",
    "\n",
    "print(\"\\na) Expected gradient:\")\n",
    "print(\"   y = x² + 3x\")\n",
    "print(\"   dy/dx = 2x + 3\")\n",
    "print(\"   At x=2: dy/dx = 2(2) + 3 = 7\")\n",
    "print(\"   Expected: tensor([7.])\")\n",
    "\n",
    "print(\"\\nb) If requires_grad=False:\")\n",
    "x_no_grad = torch.tensor([2.0], requires_grad=False)\n",
    "y_no_grad = x_no_grad**2 + 3 * x_no_grad\n",
    "print(f\"   x.grad will be None: {x_no_grad.grad}\")\n",
    "print(\"   PyTorch doesn't track operations for gradient computation\")\n",
    "\n",
    "print(\"\\nc) Without specifying requires_grad flag:\")\n",
    "x_default = torch.tensor([2.0])\n",
    "print(f\"   Default requires_grad = {x_default.requires_grad}\")\n",
    "print(\"   Gradients will NOT be tracked (default is False)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUESTION 7: Introduce Weights\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create tensors\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "w = torch.tensor([1.0, 3.0], requires_grad=True)  # Important: set requires_grad=True\n",
    "# Define function y = w[0]*x² + w[1]*x\n",
    "y = w[0] * x**2 + w[1] * x\n",
    "# Backpropagate\n",
    "y.backward()\n",
    "\n",
    "print(f\"x.grad = {x.grad}\")\n",
    "print(f\"w.grad = {w.grad}\")\n",
    "\n",
    "print(\"\\na) Original code (w without requires_grad=True):\")\n",
    "print(\"   w.grad would be None because requires_grad=False by default\")\n",
    "print(\"   The graph wasn't tracking gradients w.r.t. w\")\n",
    "\n",
    "print(\"\\nb) Modified code above shows w.grad computation\")\n",
    "print(\"   w[0].grad = dy/dw[0] = x² = 4\")\n",
    "print(\"   w[1].grad = dy/dw[1] = x = 2\")\n",
    "print(\"   Result: tensor([4., 2.])\")\n",
    "\n",
    "print(\"\\nc) Same as Q6c: Default requires_grad=False, so gradients\")\n",
    "print(\"   are NOT tracked unless explicitly set to True\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUESTION 8: Breaking the Graph\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nOriginal code:\")\n",
    "print(\"   x = torch.tensor([1.0], requires_grad=True)\")\n",
    "print(\"   y = x * 3\")\n",
    "print(\"   z = y.detach()\")\n",
    "print(\"   w = z * 2\")\n",
    "print(\"   w.backward()  # ERROR!\")\n",
    "\n",
    "print(\"\\nWhy it fails:\")\n",
    "print(\"   - .detach() breaks the computational graph\")\n",
    "print(\"   - z has no gradient information linked to x\")\n",
    "print(\"   - w depends on z, but z doesn't track gradients\")\n",
    "print(\"   - No path exists to backpropagate to x\")\n",
    "\n",
    "print(\"\\nFix 1: Don't detach if you need gradients\")\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = x * 3\n",
    "w = y * 2  # Remove detach\n",
    "w.backward()\n",
    "print(f\"   x.grad = {x.grad}  (should be 6.0)\")\n",
    "\n",
    "print(\"\\nFix 2: Use detach_() but compute loss without it:\")\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = x * 3\n",
    "z_value = y.detach().clone()  # Store the value\n",
    "# If you need w to have gradients: don't use z\n",
    "w = z_value * 2  # This won't have gradients\n",
    "# But if you recompute: w = y * 2 instead\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"QUESTION 9: Gradient Accumulation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y1 = x * 2\n",
    "y1.backward()\n",
    "print(\"After first backward: x.grad =\", x.grad)\n",
    "\n",
    "y2 = x * 3\n",
    "y2.backward()\n",
    "print(\"After second backward: x.grad =\", x.grad)\n",
    "\n",
    "print(\"\\nWhat's happening:\")\n",
    "print(\"   - First backward: x.grad = 2\")\n",
    "print(\"   - Second backward: x.grad += 3 (accumulated!)\")\n",
    "print(\"   - Result: x.grad = 5 instead of 3\")\n",
    "\n",
    "print(\"\\nWhy:\")\n",
    "print(\"   PyTorch ACCUMULATES gradients by default\")\n",
    "print(\"   This is useful for mini-batch training\")\n",
    "\n",
    "print(\"\\nHow to avoid:\")\n",
    "print(\"   Option 1: Manually zero gradients\")\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y1 = x * 2\n",
    "y1.backward()\n",
    "print(f\"   After y1.backward(): x.grad = {x.grad}\")\n",
    "x.grad.zero_()  # Clear gradients\n",
    "y2 = x * 3\n",
    "y2.backward()\n",
    "print(f\"   After zero_() and y2.backward(): x.grad = {y2.grad}\")\n",
    "\n",
    "print(\"\\n   Option 2: In training loops, use optimizer.zero_grad()\")\n",
    "print(\"   This is the standard practice in PyTorch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
